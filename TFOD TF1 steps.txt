1) Download TFOD Repo

2) Download model zoo repo

3) Design the utils folder in which you should have the images folder and Labelmap.pbtxt file.

4) Create conda environment , and activate the environment

5) pip install pillow lxml Cython contextlib2 jupyter matplotlib pandas opencv-python tensorflow==1.14.0

6) python setup.py install

7) conda install -c anaconda protobuf

8) protoc object_detection/protos/*.proto --python_out=.

9) Launch Object Detection:
-> Object_detection.utils in the import
-> In variables, Change the path to labels to object_detection/data
-> In detection, change the path to test images dir to object_detection/test_images

10) Annotate the images using labelIMG and store the xml file as PASCAL VOC format.

11) Copy the images folder from the utils and put it into research folder

12) python xml_to_csv.py

13) Inside the generate_tfrecord.py change the Label map
    python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record
    python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record
	
14) Copy the file train.py from legacy folder in object_detection to research

15) For config file get the config file from object_detection/samples/config and paste into training 

16) Copy the training folder and paste it into research folder

17) Inside Config file:
-> Change the classes
-> Copy the model folder and paste it in the research folder
-> Change the fine tune check point path 
-> Input path for both train and test record
-> Change the labelmap.pbtxt path which is avaliable in training folder

18) From research/slim copy the deployment and nets folder move to the research folder

19) Start the training:
	python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_coco.config

20) To save the inference graph:
	python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/faster_rcnn_inception_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-1000 --output_directory inference_graph

21) Prediction:

--> Open jupyter notebook
--> Inside variables instead of model name give the inference graph
--> Comment the model_file and download_base lines
--> In Path_to_lables change the path to 
	PATH_TO_LABELS = os.path.join('training', 'labelmap.pbtxt')
--> DONT RUN THE DOWNLOAD model cell
--> Run the load frozen graph cell(Load a (frozen) Tensorflow model into memory.)
--> Run all the cells 
--> In detection : Change the path (PATH_TO_TEST_IMAGES_DIR)

22) For prediction by popping up the window using cv2:

import cv2
 
cap = cv2.VideoCapture(0)
with detection_graph.as_default():
  with tf.Session(graph=detection_graph) as sess:
    while True:
      ret, image_np = cap.read()
      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
      image_np_expanded = np.expand_dims(image_np, axis=0)
      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
      # Each box represents a part of the image where a particular object was detected.
      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
      # Each score represent how level of confidence for each of the objects.
      # Score is shown on the result image, together with the class label.
      scores = detection_graph.get_tensor_by_name('detection_scores:0')
      classes = detection_graph.get_tensor_by_name('detection_classes:0')
      num_detections = detection_graph.get_tensor_by_name('num_detections:0')
      # Actual detection.
      (boxes, scores, classes, num_detections) = sess.run([boxes, scores, classes, num_detections],feed_dict={image_tensor: image_np_expanded})
      # Visualization of the results of a detection.
      vis_util.visualize_boxes_and_labels_on_image_array(
          image_np,
          np.squeeze(boxes),
          np.squeeze(classes).astype(np.int32),
          np.squeeze(scores),
          category_index,
          use_normalized_coordinates=True,
          line_thickness=8)
 
      cv2.imshow('object detection', cv2.resize(image_np, (800,600)))
      if cv2.waitKey(25) & 0xFF == ord('q'):
        cv2.destroyAllWindows()
        break
 
cap.release()


